{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6OMUZvwZtDl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import torch\n",
        "import keras\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from keras.layers import TorchModuleWrapper\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7tU6idATGhW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stuff by hand\n",
        "\n",
        "getting used to pytorch...\n",
        "\n",
        "\n",
        "code src:\n",
        "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb"
      ],
      "metadata": {
        "id": "S8cwhCR1eOAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "3KdqE4EZeXC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass steps:\n",
        "1. Generating matrices (transform input into Q, K, V matrices)\n",
        "2. calculate attn scores (matmul of query x key, normalize by softmax scaled by sqrt(embed size))\n",
        "3. apply attn - weighted sum of value vecs and attention scores thru self.fc_out"
      ],
      "metadata": {
        "id": "XlP7YvbOedyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### simple self attn"
      ],
      "metadata": {
        "id": "bc7tTcAdZp5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#self attn class\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(SimpleSelfAttention, self).__init__() #parent nn.module classf\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "\n",
        "    #linear layers (ie dense layer): args = # of input and output features; internally creates weight matrix of size (out_features, in_features) + bias of size (out_features)\n",
        "    #internal weights, biases randomly initialized\n",
        "    self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
        "    self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
        "    self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
        "    self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "\n",
        "  #forward method\n",
        "  def forward(self, value, key, query):\n",
        "    # get Q, K, V matrices; takes input vecs and apply weight/bias from init\n",
        "    #each dim is (batch_size, seq_length, embed_size)\n",
        "    queries = self.queries(query)\n",
        "    keys = self.keys(key)\n",
        "    values = self.values(value)\n",
        "\n",
        "    #calculate attention scores\n",
        "\n",
        "    #dot product bt query, key matrices\n",
        "    energy = torch.bmm(queries,\n",
        "                       keys.transpose(1, 2)) #transpose key matrix so dims align for batch mat mul, ie (batch size, seq length)\n",
        "                       #Q is (batch_size, seq_len, embed_size); last dim, embed_size, needs to match K second to last dim\n",
        "                       #so we need K to be: (batch_size, embed_size, seq_length)\n",
        "\n",
        "\n",
        "    #softmax, divide by sqrt(embed size) for stability\n",
        "    #softmax is across *last dimension* of tensor, ie sequence length of keys/ tokens in sequence\n",
        "    #now for each token (along seq length), attn scores will sum to 1 across all other tokens\n",
        "    attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
        "\n",
        "\n",
        "    #get weighted value vectors\n",
        "    out = torch.bmm(attention, values)\n",
        "\n",
        "    #apply final linear layer\n",
        "    out = self.fc_out(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "NRq5uh3SeWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### multi self attn"
      ],
      "metadata": {
        "id": "VA86FGEdZl6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    #each head should process d_model/h dimensions\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.d_model = d_model #total dimensionality of model = input embed size\n",
        "    self.num_heads = num_heads #number of heads\n",
        "    self.d_k = d_model // num_heads #dimensionality of vectors for each head to operate on\n",
        "      #input embeds projected into multiple subspaces --> each receives vector (size=d_k) for q k v ops\n",
        "\n",
        "\n",
        "\n",
        "    #initialize linear layers\n",
        "    #as above: linear layers (ie dense layer): args = # of input and output features; internally creates weight matrix of size (out_features, in_features) + bias of size (out_features)\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "    #numerator: again need to multiply q&k and transpose over last 2 dimensions for alignment\n",
        "    #denom: scale by sqrt subspace size\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    #masking: mask applied to attn scores to prevent certain tokens from attending to future tokens\n",
        "    #more efficient to apply after dot product; even if we applied mask before dot product, we would still have to normalize attn scores\n",
        "    #instead of just 0ing out, set scores to very large neg number so that positions will have near-0 probabilities after softmax\n",
        "    if mask is not None:\n",
        "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    #softmax along last dimension (= seq length of keys)\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    #weighted sum over values based on attn probabilities\n",
        "    output = torch.matmul(attn_probs, V) #output size = (batch_size, seq_len, dim_v)\n",
        "    return output\n",
        "\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    ''' split input tensor into multiple attn heads'''\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "\n",
        "    #reshape to split model dim into num_heads attn heads = size d_k\n",
        "    output = (\n",
        "        x.view(batch_size, seq_length, self.num_heads, self.d_k) #reshapes tensor w/o copying data\n",
        "         .transpose(1, 2) #for parallelization, need num_heads come before seq_length, so tensor shape = (batch_size, num_heads, seq_length, d_k)\n",
        "    )\n",
        "    return output\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    '''restore initial embed dimension '''\n",
        "    batch_size, _, seq_length, d_k = x.size() #_ standin for num_heads but we already know self.num_heads\n",
        "    output = x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "      #continguous: store data in continguous memory for valid reshaping\n",
        "      #reshape (.view) tensor back to (batch_size, seq_length, d_model) where model dim = num_heads*d_k (restore original embed dim)\n",
        "    return output\n",
        "\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    #apply weights/biases from init matrices & split into heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "      #each has shape: (batch_size, num_heads, seq_length, d_model//num_heads)\n",
        "\n",
        "\n",
        "    #apply scaled dot product attn for each head independently\n",
        "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "    #combine heads, apply output linear layer\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "      #transpose, reshape attn outputs\n",
        "      #initially: (batch_size, num_heads, seq_length, d_k)\n",
        "      #output: (batch_size, seq_len, d_model)\n",
        "      #then pass thru learned linear transform W_o (project multi head output back to original model dim embed space)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JSKdW1cAaA7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FFNN"
      ],
      "metadata": {
        "id": "vW_ZvzDBkGMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    '''2 dense + reLU activation'''\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff) #expand input dim from model dim to ffnn size (usually larger)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model) #reduce dimensionality back down to model dim\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "XaCADqLPkKC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## positional encodings"
      ],
      "metadata": {
        "id": "OKmL5HIFlCuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''use sine, cosine of diff frequencies to gen positional encoding'''\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model) #create tensor to store positional encoding vals\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        #calculate sin/cos for even/odd indices based on scaling factor div_term\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #compute positional encoding by adding stored positional encoding vals to input tensor\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "S_QE1Ik3lDsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder & Decoder"
      ],
      "metadata": {
        "id": "0egwpmNdlroG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "zVnbcisXlw3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- multi-head attn layer\n",
        "- position wise FFNN layer\n",
        "- 2 layer norm layers"
      ],
      "metadata": {
        "id": "JQDy1ZZslxrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        '''params: d_ff = dimensionality of FFNN hidden layer '''\n",
        "        #initialize layers\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model) #norm for residual connections\n",
        "        self.norm2 = nn.LayerNorm(d_model) #norm for residual connections\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "      #x =  input tensor (batch_size, seq_length, model_dim)\n",
        "      attn_output = self.self_attn(x, x, x, mask)\n",
        "        #x = q,k,v (allow each token to look at each token)\n",
        "        #output = weighted sum of vals based on attn scores\n",
        "      x = self.norm1(x + self.dropout(attn_output)) #add original input (ie residual connection), apply dropout, layer norm for stabilization\n",
        "      ff_output = self.feed_forward(x) #pass normalized output through FFNN\n",
        "      x = self.norm2(x + self.dropout(ff_output)) #add original input (ie residual connection), apply dropout, layer norm for stabilization\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "jWQ937EDl1WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n"
      ],
      "metadata": {
        "id": "mbleIeAam_vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- multi head attn x2\n",
        "- FFNN\n",
        "- layer norm x3"
      ],
      "metadata": {
        "id": "gfLYp2u0nHtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        #initialize multi head attn for self & cross attn\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        #initialize FFNN, layer norms, dropout\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        #self attn over decoder's current, previous outputs\n",
        "          #tgt_mask used to prevent from attn to future tokens\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output)) #layer norm + residual connection\n",
        "\n",
        "        #cross attn; now x=query, encoder output = keys and values\n",
        "        #src_mask = padding tokens in enc output don't go into attn scores\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output)) #residual connection, layer norm, dropout\n",
        "        ff_output = self.feed_forward(x) #cross attn output goes thru FFNN\n",
        "        x = self.norm3(x + self.dropout(ff_output)) #output layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "YbzsTdqknHHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## combine everything (transformer block)"
      ],
      "metadata": {
        "id": "O3WecbNsoIWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        #initialize embedding layers for src & tgt sequences\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        #initialize positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        #lists of encoder & decoder layers for stacked layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        #linear layer for projecting decoder output ie map decoder output to target vocab size\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        #initialize dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        #create mask for src, target inputs to identify non-padding tokens\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) #mask shape: (batch_size, 1, 1, src_length)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "\n",
        "        #no-peek: target only attend to itself & previous\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask #update target mask to include no peek mask\n",
        "\n",
        "        #output\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "\n",
        "        #generate src & target masks\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        #combine src & target embeddings + positional encodings --> dropout\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        #encoding process - pass encoder output (src_embedded) thru each encoder layer in list (attn+FFNN)\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        #take embedded target seq, process thru eachd ecoder layer\n",
        "        #use current output dec_output, encoder output enc_output to generate next output\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "        #pass thru final linear layer --> project to target vocab size, produce prob distrib\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "3NHMvsg6oUP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ij1eUryvS9B"
      }
    }
  ]
}